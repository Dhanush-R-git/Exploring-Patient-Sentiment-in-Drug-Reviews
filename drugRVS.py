# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eRNPg5LCouMwTK4V4f8QU9ds2t-75mbe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download("stopwords")
nltk.download("punkt")
nltk.download('wordnet')
nltk.download('brown')

df = pd.read_csv("/workspaces/Exploring-Patient-Sentiment-in-Drug-Reviews/datasets_drugRVS/drugTrain.csv")
df.head()

def clean_text(text):
    """Cleans text by removing unwanted symbols and digits."""
    # Remove special characters and digits
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    # Convert text to lowercase (optional)
    # text = text.lower()
    # Remove extra whitespace
    text = re.sub(r"\s+", " ", text).strip()
    return text

df["Review"] = df["Review"].apply(clean_text)
df.head()

sorted_df = df.sort_values(by="drugName")
sorted_df.head()

sorted_df["processed_review"] = ""
stop_words = set(stopwords.words("english"))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Tokenize, process, and store each review
for i, row in sorted_df.iterrows():
    review = row["Review"]

    # Tokenize the review
    tokens = word_tokenize(review)

    # Remove stop words
    tokens = [token for token in tokens if token not in stop_words]

    # Apply stemming or lemmatization (choose one or both)
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Choose either stemmed or lemmatized tokens
    processed_tokens = stemmed_tokens  # Or use lemmatized_tokens

    # Combine processed tokens back into a string
    processed_review = " ".join(processed_tokens)

    # Store the processed review in the new column
    sorted_df.loc[i, "processed_review"] = processed_review

# Save the modified DataFrame (optional)
sorted_df.to_csv("dataset_with_processed_reviews.csv", index=False)

grouped_reviews = sorted_df.groupby("drugName")["Review"]

sorted_df.head(499)

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

def polarity_scores_roberta(example):
    encoded_text = tokenizer(example, return_tensors='pt')
    output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict = {
        'roberta_neg' : scores[0],
        'roberta_neu' : scores[1],
        'roberta_pos' : scores[2]
    }
    return scores_dict

from tqdm.notebook import tqdm
res = {}
for i, row in tqdm(sorted_df.iterrows(), total=len(df)):
    try:
        text = row['processed_review']
        myid = row['UniqueID']
        roberta_result = polarity_scores_roberta(text)
        res[myid] = roberta_result
    except RuntimeError:
        print(f'Broke for id {myid}')

results_df = pd.DataFrame(res).T
results_df = results_df.reset_index().rename(columns={'index': 'UniqueID'})
results_df = results_df.merge(sorted_df, how='left')

results_df["drugName"] = results_df["drugName"].str.replace("/", "")
results_df

results_df["overall_score_ratio"] = results_df["roberta_pos"] / (results_df["roberta_pos"] + results_df["roberta_neu"] + results_df["roberta_neg"])

grouped_data = results_df[150:200].groupby("drugName")["overall_score_ratio"].mean()
print(grouped_data)
# Create a bar chart
plt.bar(grouped_data.index, grouped_data.values)
plt.xlabel("drugName")
plt.ylabel("Weighted Average Sentiment Score")
plt.title("Weighted Average Sentiment by Drug Name")
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for readability
plt.show()

import pandas as pd

def get_sentiment(drug_name, data):
    if drug_name not in data["drugName"].unique():
        raise ValueError(f"Drug name '{drug_name}' not found in the dataset.")

    # Get all scores for the drug, handle multiple entries
    drug_scores = data[data["drugName"] == drug_name]["overall_score_ratio"].tolist()
    print(drug_scores)
    # Calculate overall sentiment based on multiple scores (e.g., mean or median)
    overall_score = sum(drug_scores) / len(drug_scores)  # Using mean for example
    print(overall_score)
    if overall_score > 0.2:
        return "positive"
    else:
        return "negative"

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("humarin/chatgpt_paraphraser_on_T5_base")

model = AutoModelForSeq2SeqLM.from_pretrained("humarin/chatgpt_paraphraser_on_T5_base")

def paraphrase(
    question,
    num_beams=5,
    num_beam_groups=5,
    num_return_sequences=5,
    repetition_penalty=10.0,
    diversity_penalty=3.0,
    no_repeat_ngram_size=2,
    temperature=0.7,
    max_length=100
):
    input_ids = tokenizer(
        f'paraphrase: {question}',
        return_tensors="pt", padding="longest",
        max_length=max_length,
        truncation=True,
    ).input_ids

    outputs = model.generate(
        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,
        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,
        num_beams=num_beams, num_beam_groups=num_beam_groups,
        max_length=max_length, diversity_penalty=diversity_penalty
    )

    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    return res

s=input("Enter the drug Name: ").strip()
grouped_reviews = results_df.groupby("drugName")
for name,review in grouped_reviews:
     if s==name or s in name:
      drug_sentiment = get_sentiment(s, results_df)
      all_reviews_text = " ".join(review["Review"].tolist())
      res=paraphrase(all_reviews_text)

print("Sentiment Score: ",drug_sentiment)
print(f"Paraphrased reviews{res}")





